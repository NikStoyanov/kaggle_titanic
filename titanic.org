
#+TITLE: Titanic Kaggle Competition
#+AUTHOR: Nikola Stoyanov
#+EMAIL: nikola.stoyanov@postgrad.manchester.ac.uk
#+DATE:
#+STARTUP: showall
#+STARTUP: inlineimages
#+STARTUP: showstars

#+BEGIN_PREVIEW
Keywords: Data Science, Kaggle, Machine Learning, Python
#+END_PREVIEW

* TODO Introduction

* Data Exploration

** Imports
Set the correct python environment for this problem.
#+BEGIN_SRC emacs-lisp
(setenv "PYTHONPATH" "~/anaconda3/envs/py36/bin/python")
#+END_SRC

#+RESULTS:
: ~/anaconda3/envs/py36/bin/python

We are going to use scipy for the data fit and seaborn for the
visualization.
#+BEGIN_SRC ipython :exports both :async t :results output :session
import seaborn as sns
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

sns.set(color_codes=True)
#+END_SRC

#+RESULTS:

** Data Cleaning
Import to files.
#+BEGIN_SRC ipython :exports both :async t :results output :session
train_data = pd.read_csv('~/git/kaggle_titanic/data/train.csv')
test_data = pd.read_csv('~/git/kaggle_titanic/data/test.csv')
#+END_SRC

#+RESULTS:

#+BEGIN_SRC ipython :exports both :async t :results table :session
train_data.head()
#+END_SRC

#+RESULTS:
#+begin_example
# Out[12]:
#+BEGIN_EXAMPLE
  PassengerId  Survived  Pclass  \
  0            1         0       3
  1            2         1       1
  2            3         1       3
  3            4         1       1
  4            5         0       3
  
  Name     Sex   Age  SibSp  \
  0                            Braund, Mr. Owen Harris    male  22.0      1
  1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1
  2                             Heikkinen, Miss. Laina  female  26.0      0
  3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1
  4                           Allen, Mr. William Henry    male  35.0      0
  
  Parch            Ticket     Fare Cabin Embarked
  0      0         A/5 21171   7.2500   NaN        S
  1      0          PC 17599  71.2833   C85        C
  2      0  STON/O2. 3101282   7.9250   NaN        S
  3      0            113803  53.1000  C123        S
  4      0            373450   8.0500   NaN        S
#+END_EXAMPLE
#+end_example

#+BEGIN_SRC ipython :exports both :async t :results table :session
train_data.describe()
#+END_SRC

#+RESULTS:
#+begin_example
# Out[4]:
#+BEGIN_EXAMPLE
  PassengerId    Survived      Pclass         Age       SibSp  \
  count   891.000000  891.000000  891.000000  714.000000  891.000000
  mean    446.000000    0.383838    2.308642   29.699118    0.523008
  std     257.353842    0.486592    0.836071   14.526497    1.102743
  min       1.000000    0.000000    1.000000    0.420000    0.000000
  25%     223.500000    0.000000    2.000000   20.125000    0.000000
  50%     446.000000    0.000000    3.000000   28.000000    0.000000
  75%     668.500000    1.000000    3.000000   38.000000    1.000000
  max     891.000000    1.000000    3.000000   80.000000    8.000000
  
  Parch        Fare
  count  891.000000  891.000000
  mean     0.381594   32.204208
  std      0.806057   49.693429
  min      0.000000    0.000000
  25%      0.000000    7.910400
  50%      0.000000   14.454200
  75%      0.000000   31.000000
  max      6.000000  512.329200
#+END_EXAMPLE
#+end_example

As we can see the data has quite a few not a number values. We would
need to clean it, but first lets see how much of the data is actually
NaN.

#+BEGIN_SRC ipython :exports both :async t :results both :session
(len(train_data) - train_data.count()) / len(train_data)
#+END_SRC

#+RESULTS:
#+begin_example
# Out[13]:
#+BEGIN_EXAMPLE
  PassengerId    0.000000
  Survived       0.000000
  Pclass         0.000000
  Name           0.000000
  Sex            0.000000
  Age            0.198653
  SibSp          0.000000
  Parch          0.000000
  Ticket         0.000000
  Fare           0.000000
  Cabin          0.771044
  Embarked       0.002245
  dtype: float64
#+END_EXAMPLE
#+end_example

=0 - 0298d237-8808-42e1-83c6-828f8bc054aa
=#+end_example

As can be see 77% of the data in the Cabin column in NaN, 20% is in
Age and a few entries in Embarked. The rest is clean.

One first though is that the available data which exists in the cabin
column can introduce bias since it is likely to come from
survivors. Let us quickly check that before we proceed. We are going
to test the null hypothesis that there is no relationship between
survived people and the availability of the cabin data. The way we can
do this is using a Pearson test.

#+BEGIN_SRC ipython :exports both :async t :results table :session
  from scipy.stats import pearsonr
  pearsonr(~train_data[['Cabin']].isnull().values,
           train_data[['Survived']])
#+END_SRC

#+RESULTS:
: # Out[31]:
: : (array([0.31691152]), array([3.09089104e-22]))

#+end_example

The results are a Pearson $r=0.317$ and a $p=3.09\times10^{-22}$. The
small $p-value$ indicates that we can confirm the null
hypothesis. Even though $p-values$ are not very reliable with such a
small data range we can be more certain considering the rather small
Pearson $r$ coefficient of linear dependency. The conclusion is that,
I am confident that there was no bias in the cabin data and I can hope
that it can be infered from other features.

Now we move to the missing values in the age. We have two options on
how to fill them. The lazy approach would be a choice between mean or
median. In many applications involving statistical data there will be
noise and outliers which affects the value the mean provides. Hence,
the median would often be picked instead of the mean. In our data,
however, there is no noise since the age column contains a limited
number range i.e. age has no noise. For this reason, we can fill the
values with the mean. We can see more confirmation here.

#+BEGIN_SRC ipython :exports both :async t :results table :session
train_data['Age'].describe()
#+END_SRC

#+RESULTS:
#+begin_example
# Out[6]:
#+BEGIN_EXAMPLE
  count    714.000000
  mean      29.699118
  std       14.526497
  min        0.420000
  25%       20.125000
  50%       28.000000
  75%       38.000000
  max       80.000000
  Name: Age, dtype: float64
#+END_EXAMPLE
#+end_example

The data shows that the minimum age is about 6 months and the maximum
age is 80 years which tells us that no crazy outliers are hidding in
the data to affect the mean. So the lazy approach can be done with
filling the mean. However, we can hope for something better which
leads us to option 2 - the smart approach. We can hope to find an
estimate for the missing age values by looking at other features. I am
an optimist and will go for option 2.

Finally, the missing data in the embarked column is so small that I
will just filter it out.

#+BEGIN_SRC ipython :exports both :async t :results result :session
train_data = train_data.drop(['Cabin'], axis=1)
#+END_SRC

#+RESULTS:
: # Out[14]:

To summarise the missing age and cabin values will be filled by
looking at other features. Let's analyse the data now and see what
they can be. Furhtermore, the analysis will help us later to engineer
new features and hopefully make better predictions.

** TODO Analysis
Lets explore some of the data to get a sense of what is going on. We
are going to look at the age, male/female, ticket fare, embarkment
city and family data and how it relates to survivability.

To facilitate the analysis we need to first remove the remaining
missing values (before figuring out how to fill them). This can be
done as below.

#+BEGIN_SRC ipython :exports both :async t :results table :session
clean_train_data = train_data.dropna(subset=['Age', 'Embarked'])
(len(clean_train_data) - clean_train_data.count()) / len(clean_train_data)
#+END_SRC

#+RESULTS:
#+begin_example
# Out[15]:
#+BEGIN_EXAMPLE
  PassengerId    0.0
  Survived       0.0
  Pclass         0.0
  Name           0.0
  Sex            0.0
  Age            0.0
  SibSp          0.0
  Parch          0.0
  Ticket         0.0
  Fare           0.0
  Embarked       0.0
  dtype: float64
#+END_EXAMPLE
#+end_example

*** Age
We can start by exporing the relationship between age and
survivability. We can make a null hypothesis assumption that the
younger you are the more chance you have of survival. Having watched
the Titanic movie this statement certainly makes sense in my head but
we cannot blindly trust James Cameron can we?

We can check if he did his homework by plotting the histograms of
survival (0 Nope / 1 Yep) for age and then compare the kernel density
estimate (KDE). From the KDE we can estimate the probability density
function (PDF) of the the age-survival random variable.

Lets plot the histograms. First the total distribution of all
passangers and then the passangers than survived.

#+BEGIN_SRC ipython :session :ipyfile img/sns_dist_age.png :exports both :async t :results raw drawer
fig = plt.figure()
sns.distplot(clean_train_data['Age'], bins=20, kde=False)
fig.tight_layout()
plt.show()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[17]:
[[file:img/sns_dist_age.png]]
:END:

#+BEGIN_SRC ipython :session :ipyfile img/sns_dist_age_surv1.png :exports both :async t :results raw drawer
fig = plt.figure()
sns.distplot(clean_train_data['Age'][clean_train_data['Survived'] == 1], bins=20, kde=False)
fig.tight_layout()
plt.show()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[18]:
[[file:img/sns_dist_age_surv1.png]]
:END:

#+LATEX_ATTR: :placement [H]
#+CAPTION: Age histogram of Survival = 1 from train data
#+NAME: sns_dist_age_surv1

To compute the KDE seaborn will put a Gaussian distribution centered
at each bin with predefined parameters and then sum them. It will then
normalize the result so that the integral is equal to unity. In a way
it will smooth the data which is dictated by the bandwith parameter
(bw in python). This is what controls the trade-off between the bias
and variance of the estimator. We are going to leave the default
bandwith here, see how well it works and determine if we need to play
with it.

Lets plot the two KDE and compare them.

#+BEGIN_SRC ipython :session :ipyfile img/sns_kde_age.png :exports both :async t :results raw drawer
fig = plt.figure()
sns.kdeplot(clean_train_data['Age'], label='Total')
sns.kdeplot(clean_train_data['Age'][clean_train_data['Survived'] == 1], label='Survived = 1')
fig.tight_layout()
plt.show()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[19]:
[[file:img/sns_kde_age.png]]
:END:

The integral of both curves is equal to unity so we can directly
compare them. From the plot we can do a quick qualitative comparison
(high/low) and what we can see is that:

- Age < 10 (Group 1): High proportion survived,
- 10 < Age < 30 (Group 2): Low proportion survived,
- 30 < Age < 60 (Group 3): High proportion survived,
- Age > 60 (Group 4): Low proportion survived.

What this data tells us is that survivability is correlated with age
in a categorical manner and not absolute. Or in other words people in
Group 1 were prioratised, Group 2 and 3 were not prioratised and Group
4 was in between. This statement certainly makes sense (thank you Mr
Cameron), however, we need to look into the male/female distribution
and the family relations in order to comment more on how this
information can help us to fill the missing age values.

*** TODO Male/Female

#+BEGIN_SRC ipython :exports both :async t :results output :session

#+END_SRC

*** TODO Family

*** TODO Ticket Fare
#+BEGIN_SRC ipython :session :ipyfile img/ticket_price_age.png :exports both :async t :results raw drawer
fig = plt.figure()
sns.jointplot(data=train_data, x='Age', y='Fare', kind='reg')
fig.tight_layout()
plt.show()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[175]:
[[file:img/ticket_price_age.png]]
:END:

#+BEGIN_SRC ipython :session :ipyfile img/ticket_fare.png :exports both :async t :results raw drawer
fig = plt.figure()
g = sns.FacetGrid(train_data, row='Survived', col='Pclass')
g.map(sns.distplot, "Age")
fig.tight_layout()
plt.show()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[176]:
[[file:img/ticket_fare.png]]
:END:


*** TODO Correlation
#+BEGIN_SRC ipython :session :ipyfile img/corr_heatmap.png :exports both :async t :results raw drawer
fig = plt.figure()
sns.heatmap(train_data.corr(), annot=True, fmt=".2f")
fig.tight_layout()
plt.show()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[177]:
[[file:img/corr_heatmap.png]]
:END:

** TODO Data Standartisation

During the data exploration we noticed that some of the values are categorical i.e.
male/female. To facilitate the use of this date in machine learning models we need to
encode the data with a method such as the one-hot encoder.

For this we can use the preprocessing module of scikit. Lets see again
which columns would require an encoding.
#+BEGIN_SRC ipython :exports both :async t :result table :session
from sklearn import preprocessing

train_data.columns
#+END_SRC

#+RESULTS:
: # Out[14]:
: #+BEGIN_EXAMPLE
:   Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',
:   'Parch', 'Ticket', 'Fare', 'Embarked'],
:   dtype='object')
: #+END_EXAMPLE

It seems that we would need to encode the columns: 'Sex', 'Parch' and 'Embarked'.

First lets place integer labels for the categorical data.
#+BEGIN_SRC ipython :exports both :async t :results table :session
enc_labels = preprocessing.LabelEncoder()
training_cat_labels = train_data[['Sex', 'Parch', 'Embarked']].apply(enc_labels.fit_transform)
testing_cat_labels = test_data[['Sex', 'Parch', 'Embarked']].apply(enc_labels.fit_transform)
training_cat_labels.head()
#+END_SRC

#+RESULTS:
: # Out[21]:
: #+BEGIN_EXAMPLE
:   Sex  Parch  Embarked
:   0    1      0         2
:   1    0      0         0
:   2    0      0         2
:   3    0      0         2
:   4    1      0         2
: #+END_EXAMPLE

For SVM we would also need to use the one hot encoder.

#+BEGIN_SRC ipython :exports both :async t :results table :session
onehot = preprocessing.OneHotEncoder()

onehot.fit(training_cat_labels)
onehot_labels = onehot.transform(training_cat_labels).toarray()
onehot_labels
#+END_SRC

#+RESULTS:
#+begin_example
# Out[23]:
#+BEGIN_EXAMPLE
  array([[0., 1., 1., ..., 0., 0., 1.],
  [1., 0., 1., ..., 1., 0., 0.],
  [1., 0., 1., ..., 0., 0., 1.],
  ...,
  [1., 0., 1., ..., 0., 0., 1.],
  [0., 1., 1., ..., 1., 0., 0.],
  [0., 1., 1., ..., 0., 1., 0.]])
#+END_EXAMPLE
#+end_example

#+end_example

The labels to make sense - we now have discrete integer values for the
categorical features! Next lets create the one-hot encoder and
transform the values.

*** TODO Make the data to a gaussian with zero mean and unit variance.

* TODO Feature Engineering

* TODO Logistic Regression Functions
Since we are going to explore different classification algorithms
we need to have easy access to perform the checks. Lets write some
functions to make this handling easier.

Coolest thing even in org-mode C-c ' will open a crazy cool buffer to edit code.

#+BEGIN_SRC ipython :exports both :async t :results output :session
  def make_meshgrid(x_data, y_data, h_step=0.02):
      """ Create a grid of points. From:
      http://scikit-learn.org/stable/auto_examples/svm/plot_iris.html

      Args:
          x: data to base x-axis meshgrid on
          y: data to base y-axis meshgrid on
          h: step size

      Outputs:
          x_mesh, y_mesh: ndarray with the grid
      """
      x_min, x_max = x_data.min() - 1, x_data.max() + 1
      y_min, y_max = y_data.min() - 1, y_data.max() + 1

      x_mesh, y_mesh = np.meshgrid(np.arange(x_min, x_max, h_step),
                                   np.arange(y_min, y_max, h_step))
      return x_mesh, y_mesh
#+END_SRC

#+RESULTS:

* TODO Support Vector Machines
From the scikit documentation the C-Support Vector Classification has a
more than quadratic time complexity for the fit and scaling is difficult
with datasets of more than $10^4$ samples. Luckily our dataset is much smaller.

#+BEGIN_SRC ipython :exports both :async t :results table :session
  from sklearn import svm

  clf = svm.SVC()
  #clf.fit(train_data['Age'].values.reshape(-1, 1), train_data['Survived'].values.reshape(-1, ))
  #prediction = clf.predict(test_data['Age'].values.reshape(-1, 1))
  clf.fit(train_data.loc[:, ['Age','Fare', 'Pclass', 'SibSp', 'Sex'], train_data['Survived'].values.reshape(-1, ))
  #prediction = clf.predict(test_data.loc[:, ['Age', 'Fare']])

  #write this to file
  #output = test_data.loc[:, ['PassengerId']]
  #output['Survived'] = prediction
  #output.to_csv('SVM_age_fare.csv', index=False)
  #output
#+END_SRC

#+RESULTS:
: # Out[180]:
: #+BEGIN_EXAMPLE
:   SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
:   decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',
:   max_iter=-1, probability=False, random_state=None, shrinking=True,
:   tol=0.001, verbose=False)
: #+END_EXAMPLE

To submit
sh :results value
kaggle competitions submit -c titanic -f SVM_age_fare.csv -m "Trial submission with SVM and two features"

#+RESULTS:
: Successfully submitted to Titanic: Machine Learning from Disaster

* TODO Random Forests

#+BEGIN_SRC ipython :exports both :async t :results output :session
from sklearn.ensemble import RandomForestClassifier

clf = RandomForestClassifier(max_depth=2, random_state=0)
clf.fit(training_cat_labels.loc[:, ['Sex', 'Parch', 'Embarked']], train_data['Survived'].values.reshape(-1, ))
print(clf.feature_importances_)
#+END_SRC

#+RESULTS:
: [0.52487645 0.19963666 0.27548689]

#+BEGIN_SRC ipython :exports both :async t :results output :session
rf_classifier = clf.predict(testing_cat_labels)
rf_data = np.vstack((test_data['PassengerId'].values, rf_classifier))

np.savetxt('rf_data.csv', rf_data.T, delimiter=',', fmt='%.f', header='PassengerId,Survived', comments='')
#+END_SRC

#+RESULTS:

#+BEGIN_SRC sh :results value
kaggle competitions submit -c titanic -f rf_data.csv -m "Random Forest trial submission"
#+END_SRC

#+RESULTS:
| Warning:     | Looks     | like | you're   | using   | an       | outdated | API      | Version, | please | consider | updating | (server | 1.3.8 | / | client | 1.3.6) |
| Successfully | submitted | to   | Titanic: | Machine | Learning | from     | Disaster |          |        |          |          |         |       |   |        |        |

* Comments
#+BEGIN_HTM
<div id='disqus_thread'></div>
<script>
    var disqus_config = function () {
        this.page.url = 'https://niksto.net/titanic.html';
        this.page.identifier = '7099f7ff-dc02-4829-9064-75875a5daca4';
        this.page.title = 'Kaggle - Titanic - Data Science';
    };
    (function() {
        var d = document, s = d.createElement('script');
        s.src = 'https://niksto-net.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>
    Please enable JavaScript to view the
    <a href='https://disqus.com/ref_noscript' rel='nofollow'>
        comments powered by Disqus.
    </a>
</noscript>
#+END_HTM
